# Persistent AI Analysis System - FastAPI + Async SQLAlchemy Template

A production-ready template for building continuously running, knowledge-accumulating AI analysis systems using modern 2024-2025 best practices.

## Architecture Overview

This template provides a robust foundation for:
- ğŸ“ˆ **Continuous Learning**: System that accumulates knowledge over time
- ğŸ”„ **Persistent State**: All analysis results and insights stored in database
- âš¡ **High Performance**: Async FastAPI + SQLAlchemy for concurrent processing
- ğŸ›¡ï¸ **Production Ready**: Docker, monitoring, error handling, and scalability
- ğŸ”— **API Integration**: Reliable external API integration with retry logic
- ğŸ“Š **Background Tasks**: Long-running analysis jobs with Celery/Redis

## Key Features

- **Async-First Architecture**: Full async/await pattern throughout
- **Persistent Knowledge Base**: All insights and analysis stored in PostgreSQL
- **Modular Design**: Clean separation of concerns with dependency injection
- **Robust Error Handling**: Circuit breakers, retries, and graceful degradation
- **Scalable Background Tasks**: Celery with Redis for distributed processing
- **Monitoring & Observability**: Prometheus metrics, structured logging
- **Container-Ready**: Docker composition for easy deployment

## Tech Stack

### Core Framework
- **FastAPI**: Async web framework with automatic OpenAPI docs
- **SQLAlchemy 2.0+**: Async ORM with connection pooling
- **PostgreSQL**: Primary database with asyncpg driver
- **Alembic**: Database migrations

### Background Processing
- **Celery**: Distributed task queue for long-running jobs
- **Redis**: Message broker and caching layer
- **APScheduler**: Periodic task scheduling

### External Integrations
- **httpx**: Async HTTP client with retry logic
- **Pydantic**: Data validation and settings management

### Production & Monitoring
- **Docker**: Containerization
- **Prometheus**: Metrics collection
- **Grafana**: Monitoring dashboards
- **Gunicorn + Uvicorn**: Production ASGI server

## Project Structure

```
src/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ config.py              # Configuration management
â”œâ”€â”€ database.py            # Database connection and session management
â”œâ”€â”€ dependencies.py        # Dependency injection providers
â”œâ”€â”€ middleware.py          # Custom middleware (CORS, logging, etc.)
â”œâ”€â”€ exceptions.py          # Custom exception handlers
â”œâ”€â”€ 
â”œâ”€â”€ models/                # SQLAlchemy models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py           # Base model class
â”‚   â”œâ”€â”€ document.py       # Document analysis models
â”‚   â”œâ”€â”€ insight.py        # Knowledge and insights models
â”‚   â””â”€â”€ task.py           # Background task tracking
â”‚
â”œâ”€â”€ schemas/              # Pydantic schemas
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document.py
â”‚   â”œâ”€â”€ insight.py
â”‚   â””â”€â”€ task.py
â”‚
â”œâ”€â”€ api/                  # API routes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ documents.py  # Document upload/analysis endpoints
â”‚   â”‚   â”œâ”€â”€ insights.py   # Knowledge retrieval endpoints
â”‚   â”‚   â”œâ”€â”€ analysis.py   # Analysis control endpoints
â”‚   â”‚   â””â”€â”€ health.py     # Health check endpoints
â”‚
â”œâ”€â”€ services/             # Business logic layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_service.py
â”‚   â”œâ”€â”€ analysis_service.py
â”‚   â”œâ”€â”€ insight_service.py
â”‚   â””â”€â”€ external_apis.py  # External API integrations
â”‚
â”œâ”€â”€ tasks/                # Celery background tasks
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ document_tasks.py
â”‚   â””â”€â”€ analysis_tasks.py
â”‚
â”œâ”€â”€ utils/                # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â”œâ”€â”€ retry.py
â”‚   â””â”€â”€ metrics.py
â”‚
â””â”€â”€ tests/                # Test suite
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ conftest.py       # Pytest configuration
    â”œâ”€â”€ test_api/
    â”œâ”€â”€ test_services/
    â””â”€â”€ test_tasks/
```

## Quick Start

1. **Clone and Setup**
   ```bash
   git clone <this-template>
   cd fastapi-ai-system
   cp .env.example .env
   # Edit .env with your configuration
   ```

2. **Start Development Environment**
   ```bash
   docker-compose up -d
   # This starts PostgreSQL, Redis, and your app
   ```

3. **Run Migrations**
   ```bash
   docker-compose exec app alembic upgrade head
   ```

4. **Access the Application**
   - API: http://localhost:8000
   - Docs: http://localhost:8000/docs
   - Grafana: http://localhost:3000

## Configuration

All configuration is managed through environment variables and Pydantic settings:

```python
# config.py
class Settings(BaseSettings):
    # Database
    database_url: str = "postgresql+asyncpg://user:pass@localhost/db"
    
    # Redis
    redis_url: str = "redis://localhost:6379/0"
    
    # External APIs
    openai_api_key: str = ""
    anthropic_api_key: str = ""
    
    # Performance
    db_pool_size: int = 20
    max_overflow: int = 0
    
    class Config:
        env_file = ".env"
```

## Best Practices Implemented

### 1. Database Session Management
- Async session factory with dependency injection
- Context-managed session lifecycle
- Proper connection pooling and cleanup

### 2. Background Task Processing
- Celery for distributed, fault-tolerant task execution
- Task result persistence in database
- Retry logic with exponential backoff

### 3. External API Integration
- Async HTTP client with circuit breaker pattern
- Request/response logging and metrics
- Rate limiting and quota management

### 4. Error Handling & Monitoring
- Structured logging with correlation IDs
- Prometheus metrics for all operations
- Health checks for dependencies

### 5. Testing Strategy
- Async test client for API testing
- Database fixtures for clean test state
- Mock external API dependencies

## Deployment

### Development
```bash
docker-compose up -d
```

### Production
```bash
# Using Docker Swarm or Kubernetes
docker-compose -f docker-compose.prod.yml up -d
```

## Monitoring

The template includes a complete observability stack:

- **Metrics**: Prometheus scrapes metrics from `/metrics` endpoint
- **Dashboards**: Grafana with pre-configured dashboards
- **Logs**: Structured JSON logging with correlation IDs
- **Health Checks**: `/health` endpoint monitors all dependencies

## Scaling Considerations

1. **Horizontal Scaling**: Stateless app design allows multiple instances
2. **Database**: Connection pooling and read replicas
3. **Background Tasks**: Celery workers can be scaled independently
4. **Caching**: Redis for session storage and computed results
5. **Load Balancing**: Nginx reverse proxy for production

## Security Features

- CORS configuration
- Request rate limiting
- Input validation with Pydantic
- SQL injection prevention via ORM
- Secrets management via environment variables

---

This template provides a solid foundation for building production-ready, continuously learning AI analysis systems. It incorporates the latest 2024-2025 best practices while remaining flexible for your specific use case.
